services:
  mlflow-server:
    image: ghcr.io/mlflow/mlflow:v2.13.2
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlruns/mlflow.db
      --default-artifact-root /mlruns
      --serve-artifacts
    networks: # <-- Add this
      - mlops-net

  mlflow-serving:
    image: ghcr.io/mlflow/mlflow:v2.13.2
    ports:
      - "8001:8001"
      - "8081:8081"
    volumes:
      - ./mlruns:/mlruns
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000
      - MODEL_URI=models:/HeartDiseaseClassifier/Production
    command: mlflow models serve --model-uri ${MODEL_URI} --host 0.0.0.0 --port 8001 --enable-mlserver
    depends_on:
      - mlflow-server
    networks:
      - mlops-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "7860:7860"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-server:5000
    volumes:
      - ./data:/app/data
      - ./scaler.joblib:/app/scaler.joblib
      - ./imputer.joblib:/app/imputer.joblib
    depends_on:
      - mlflow-server
      - mlflow-serving
    networks: # <-- Add this
      - mlops-net

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - mlflow-serving
    networks: # <-- Add this
      - mlops-net

  grafana:
    image: grafana/grafana-oss:latest
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks: # <-- Add this
      - mlops-net

# Add this entire block to the end of the file
networks:
  mlops-net:
    driver: bridge
